{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications import ResNet50, InceptionV3, Xception\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"birds_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   2.46026177e-09   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   3.18476114e-25\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "from scipy.misc import imresize\n",
    "h,w = 224, 224\n",
    "img = load_img('test/008.jpg')\n",
    "img = img_to_array(img)\n",
    "hr = 1 / img.shape[0]\n",
    "wr = 1 / img.shape[1]\n",
    "img = imresize(img, (h, w), interp='bilinear').reshape(1,h,w,3)\n",
    "key = model.predict(img,1)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications import ResNet50, InceptionV3, Xception\n",
    "from keras import backend as K\n",
    "from keras import __version__\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ActivityRegularization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "from math import cos,sin\n",
    "from keras.preprocessing import image\n",
    "from scipy import misc\n",
    "from scipy.misc import imresize\n",
    "from os.path import join\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def read_csv(filename):\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            filename, class_id = line.rstrip('\\n').split(',')\n",
    "            res[filename] = int(class_id)\n",
    "    return res\n",
    "\n",
    "# def save_csv(img_classes, filename):\n",
    "#     with open(filename, 'w') as fhandle:\n",
    "#         print('filename,class_id', file=fhandle)\n",
    "#         for filename in sorted(img_classes.keys()):\n",
    "#             print('%s,%d' % (filename, img_classes[filename]), file=fhandle)\n",
    "            \n",
    "img_rows, img_cols = 224, 224\n",
    "nb_classes = 50\n",
    "\n",
    "aver = np.array([ 0.49004534 , 0.5139673  , 0.46879493])\n",
    "disp = np.array([ 0.23112116 , 0.22527517 , 0.26772715])\n",
    "\n",
    "def random_indices(batchsize, ratio):\n",
    "    size = int(batchsize * ratio)\n",
    "    return np.random.choice(batchsize, size, replace=False)\n",
    "\n",
    "def flip(inputs, batchsize, flip_size):\n",
    "    \"\"\"Flip image batch\"\"\"\n",
    "    indices = random_indices(batchsize, flip_size)\n",
    "#     indices = len(inputs)\n",
    "#     flip_input = inputs[indices,:,::-1]\n",
    "    for k in indices:\n",
    "        inputs[k] = inputs[k,:,::-1]\n",
    "\n",
    "\n",
    "def rotate(inputs, batchsize, rotate_size):\n",
    "    indices = random_indices(batchsize, rotate_size)\n",
    "#     indices = len(inputs)\n",
    "    for k in indices:\n",
    "        angle = np.random.randint(-10, 10)\n",
    "        inputs[k] = misc.imrotate(inputs[k, :, :, :], angle)\n",
    "        inputs[k] = inputs[k].astype('float32')/255.\n",
    "\n",
    "\n",
    "def crop(inputs, batchsize, crop_size):\n",
    "    indices = random_indices(batchsize, crop_size)\n",
    "#     indices = len(inputs)\n",
    "    for k in indices:\n",
    "        crop_x = np.random.randint(1, 10) \n",
    "        crop_y = np.random.randint(1, 10)\n",
    "#         print(crop_x)\n",
    "        imm = np.asarray(inputs[k, crop_y:, crop_x:,:])\n",
    "        imm = imresize(imm, (img_rows, img_cols))\n",
    "        imm = imm.astype('float32')/255.\n",
    "        inputs[k] = imm\n",
    "        \n",
    "\n",
    "def yagenerator(folder, gt, batch_size, augment=False):\n",
    "    h, w = 224, 224\n",
    "    names = list(gt.keys())\n",
    "#     names = names[:5000]\n",
    "    num = len(names)\n",
    "    #x = np.empty((batch_size, img_rows, img_cols, 3))\n",
    "    #y = np.empty((batch_size, 28))\n",
    "    \n",
    "    while(True):\n",
    "        random.shuffle(names)\n",
    "        for i in range(num // batch_size - 1):\n",
    "            x = np.empty((batch_size, img_rows, img_cols, 3))\n",
    "            y = np.zeros((batch_size, nb_classes))\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                ind = i * batch_size + j\n",
    "                name = names[ind]\n",
    "                #img = imread(join(folder, names[i]))\n",
    "                img = image.load_img(join(folder, name))\n",
    "                img = image.img_to_array(img)\n",
    "                hr = 1 / img.shape[0]\n",
    "                wr = 1 / img.shape[1]\n",
    "                img = imresize(img, (h, w), interp='bilinear')\n",
    "                x[j] = (img.astype('float32'))/255.\n",
    "                y [j, gt[name]] = 1\n",
    "#                 y[j] = gt[name]\n",
    "            if augment == True:\n",
    "                flip(x, batch_size, 0.7)\n",
    "                rotate(x, batch_size, 0.6)\n",
    "                crop(x, batch_size, 0.7)\n",
    "            for col in range(0, 3):\n",
    "                x[:,:,:,col] -= aver[col]\n",
    "                x[:,:,:,col] /= disp[col]\n",
    "            yield (x, y)\n",
    "\n",
    "def train_classifier(train_gt, train_img_dir, fast_train=True):\n",
    "    nb_train_samples = 2250\n",
    "    nb_val_samples = 250\n",
    "    nb_epoch = 20\n",
    "    batch_size = 16\n",
    "    img_height, img_width =  224, 224\n",
    "    #train_data_dir = '/home/ec2-user/mynotebooks/train'\n",
    "    #test_data_dir = '/home/ec2-user/mynotebooks/test'\n",
    "    #train_gt = read_csv(join(train_data_dir, 'gt.csv'))\n",
    "    #test_gt = read_csv(join(test_data_dir, 'gt.csv'))\n",
    "\n",
    "    base_model = ResNet50(weights=\"imagenet\", input_shape=(img_height, img_width, 3), include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # x = Dense(1024, activation='relu')(x) #new FC layer, random init\n",
    "    # predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    # model = Model(input=base_model.input, output=predictions)\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"compiled\")\n",
    "    if fast_train == True:\n",
    "        hist = model.fit_generator(\n",
    "        yagenerator(train_img_dir, train_gt, batch_size=batch_size, augment=False),\n",
    "        nb_epoch=1,\n",
    "        steps_per_epoch=1)\n",
    "    else:\n",
    "        hist = model.fit_generator(\n",
    "        yagenerator(train_img_dir, train_gt, batch_size=batch_size, augment=True),\n",
    "        nb_epoch=nb_epoch,\n",
    "        steps_per_epoch=nb_train_samples//batch_size)\n",
    "    return model\n",
    "\n",
    "def classify(model, test_img_dir):\n",
    "    fnames = [name for name in os.listdir(test_img_dir) if name.endswith(\".jpg\")]\n",
    "    points = dict.fromkeys(fnames)\n",
    "    \n",
    "    batch_size = 16\n",
    "    img_cols, img_rows = 224, 224\n",
    "    \n",
    "    for i in range(len(points) // batch_size + 1):\n",
    "        x = np.empty((batch_size, img_rows, img_cols, 3))\n",
    "        y = np.empty((batch_size, 50))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            ind = i * batch_size + j\n",
    "            if ind >= len(points):\n",
    "                break\n",
    "            name = fnames[ind]\n",
    "            #img = imread(join(folder, names[i]))\n",
    "            img = image.load_img(join(test_img_dir, name))\n",
    "            img = image.img_to_array(img)\n",
    "            hr = 1 / img.shape[0]\n",
    "            wr = 1 / img.shape[1]\n",
    "            img = imresize(img, (img_cols, img_rows), interp='bilinear')\n",
    "            x[j] = (img.astype('float32'))/255.\n",
    "            \n",
    "#             print(x.shape, aver.shape)\n",
    "        for col in range(0,3):\n",
    "            x[:,:,:,col] -= aver[col]\n",
    "            x[:,:,:,col] /= disp[col]\n",
    "        y = model.predict(x, batch_size, 0) \n",
    "        for j in range(batch_size):\n",
    "            ind = i * batch_size + j\n",
    "            if ind >= len(points):\n",
    "                break\n",
    "            name = fnames[ind]\n",
    "            points[name] = np.argmax(y[j])\n",
    "    return points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0045.jpg': 0,\n",
       " '0046.jpg': 0,\n",
       " '0047.jpg': 0,\n",
       " '0048.jpg': 0,\n",
       " '0049.jpg': 0,\n",
       " '0095.jpg': 1,\n",
       " '0096.jpg': 1,\n",
       " '0097.jpg': 1,\n",
       " '0098.jpg': 1,\n",
       " '0099.jpg': 1,\n",
       " '0145.jpg': 2,\n",
       " '0146.jpg': 2,\n",
       " '0147.jpg': 2,\n",
       " '0148.jpg': 2,\n",
       " '0149.jpg': 7,\n",
       " '0195.jpg': 25,\n",
       " '0196.jpg': 3,\n",
       " '0197.jpg': 3,\n",
       " '0198.jpg': 3,\n",
       " '0199.jpg': 3,\n",
       " '0245.jpg': 4,\n",
       " '0246.jpg': 25,\n",
       " '0247.jpg': 10,\n",
       " '0248.jpg': 8,\n",
       " '0249.jpg': 4,\n",
       " '0295.jpg': 5,\n",
       " '0296.jpg': 5,\n",
       " '0297.jpg': 5,\n",
       " '0298.jpg': 5,\n",
       " '0299.jpg': 5,\n",
       " '0345.jpg': 6,\n",
       " '0346.jpg': 6,\n",
       " '0347.jpg': 30,\n",
       " '0348.jpg': 6,\n",
       " '0349.jpg': 30,\n",
       " '0395.jpg': 7,\n",
       " '0396.jpg': 48,\n",
       " '0397.jpg': 7,\n",
       " '0398.jpg': 7,\n",
       " '0399.jpg': 7,\n",
       " '0445.jpg': 8,\n",
       " '0446.jpg': 8,\n",
       " '0447.jpg': 8,\n",
       " '0448.jpg': 12,\n",
       " '0449.jpg': 8,\n",
       " '0495.jpg': 12,\n",
       " '0496.jpg': 25,\n",
       " '0497.jpg': 9,\n",
       " '0498.jpg': 4,\n",
       " '0499.jpg': 5,\n",
       " '0545.jpg': 9,\n",
       " '0546.jpg': 25,\n",
       " '0547.jpg': 25,\n",
       " '0548.jpg': 9,\n",
       " '0549.jpg': 4,\n",
       " '0595.jpg': 9,\n",
       " '0596.jpg': 11,\n",
       " '0597.jpg': 12,\n",
       " '0598.jpg': 11,\n",
       " '0599.jpg': 25,\n",
       " '0645.jpg': 12,\n",
       " '0646.jpg': 44,\n",
       " '0647.jpg': 12,\n",
       " '0648.jpg': 11,\n",
       " '0649.jpg': 11,\n",
       " '0695.jpg': 13,\n",
       " '0696.jpg': 16,\n",
       " '0697.jpg': 16,\n",
       " '0698.jpg': 46,\n",
       " '0699.jpg': 7,\n",
       " '0745.jpg': 14,\n",
       " '0746.jpg': 14,\n",
       " '0747.jpg': 14,\n",
       " '0748.jpg': 14,\n",
       " '0749.jpg': 14,\n",
       " '0795.jpg': 32,\n",
       " '0796.jpg': 15,\n",
       " '0797.jpg': 15,\n",
       " '0798.jpg': 15,\n",
       " '0799.jpg': 15,\n",
       " '0845.jpg': 16,\n",
       " '0846.jpg': 49,\n",
       " '0847.jpg': 16,\n",
       " '0848.jpg': 16,\n",
       " '0849.jpg': 16,\n",
       " '0895.jpg': 49,\n",
       " '0896.jpg': 17,\n",
       " '0897.jpg': 17,\n",
       " '0898.jpg': 17,\n",
       " '0899.jpg': 40,\n",
       " '0945.jpg': 19,\n",
       " '0946.jpg': 18,\n",
       " '0947.jpg': 18,\n",
       " '0948.jpg': 18,\n",
       " '0949.jpg': 18,\n",
       " '0995.jpg': 19,\n",
       " '0996.jpg': 19,\n",
       " '0997.jpg': 19,\n",
       " '0998.jpg': 32,\n",
       " '0999.jpg': 19,\n",
       " '1045.jpg': 20,\n",
       " '1046.jpg': 20,\n",
       " '1047.jpg': 20,\n",
       " '1048.jpg': 20,\n",
       " '1049.jpg': 20,\n",
       " '1095.jpg': 21,\n",
       " '1096.jpg': 0,\n",
       " '1097.jpg': 21,\n",
       " '1098.jpg': 21,\n",
       " '1099.jpg': 37,\n",
       " '1145.jpg': 22,\n",
       " '1146.jpg': 22,\n",
       " '1147.jpg': 22,\n",
       " '1148.jpg': 22,\n",
       " '1149.jpg': 22,\n",
       " '1195.jpg': 23,\n",
       " '1196.jpg': 23,\n",
       " '1197.jpg': 23,\n",
       " '1198.jpg': 23,\n",
       " '1199.jpg': 23,\n",
       " '1245.jpg': 24,\n",
       " '1246.jpg': 24,\n",
       " '1247.jpg': 24,\n",
       " '1248.jpg': 24,\n",
       " '1249.jpg': 24,\n",
       " '1295.jpg': 25,\n",
       " '1296.jpg': 25,\n",
       " '1297.jpg': 25,\n",
       " '1298.jpg': 25,\n",
       " '1299.jpg': 25,\n",
       " '1345.jpg': 26,\n",
       " '1346.jpg': 27,\n",
       " '1347.jpg': 27,\n",
       " '1348.jpg': 29,\n",
       " '1349.jpg': 26,\n",
       " '1395.jpg': 27,\n",
       " '1396.jpg': 27,\n",
       " '1397.jpg': 28,\n",
       " '1398.jpg': 27,\n",
       " '1399.jpg': 27,\n",
       " '1445.jpg': 28,\n",
       " '1446.jpg': 28,\n",
       " '1447.jpg': 28,\n",
       " '1448.jpg': 28,\n",
       " '1449.jpg': 22,\n",
       " '1495.jpg': 29,\n",
       " '1496.jpg': 29,\n",
       " '1497.jpg': 29,\n",
       " '1498.jpg': 29,\n",
       " '1499.jpg': 29,\n",
       " '1545.jpg': 30,\n",
       " '1546.jpg': 46,\n",
       " '1547.jpg': 30,\n",
       " '1548.jpg': 30,\n",
       " '1549.jpg': 30,\n",
       " '1595.jpg': 32,\n",
       " '1596.jpg': 31,\n",
       " '1597.jpg': 31,\n",
       " '1598.jpg': 31,\n",
       " '1599.jpg': 31,\n",
       " '1645.jpg': 32,\n",
       " '1646.jpg': 32,\n",
       " '1647.jpg': 32,\n",
       " '1648.jpg': 32,\n",
       " '1649.jpg': 32,\n",
       " '1695.jpg': 33,\n",
       " '1696.jpg': 7,\n",
       " '1697.jpg': 16,\n",
       " '1698.jpg': 42,\n",
       " '1699.jpg': 33,\n",
       " '1745.jpg': 34,\n",
       " '1746.jpg': 38,\n",
       " '1747.jpg': 39,\n",
       " '1748.jpg': 36,\n",
       " '1749.jpg': 38,\n",
       " '1795.jpg': 35,\n",
       " '1796.jpg': 34,\n",
       " '1797.jpg': 35,\n",
       " '1798.jpg': 35,\n",
       " '1799.jpg': 35,\n",
       " '1845.jpg': 46,\n",
       " '1846.jpg': 36,\n",
       " '1847.jpg': 38,\n",
       " '1848.jpg': 36,\n",
       " '1849.jpg': 36,\n",
       " '1895.jpg': 37,\n",
       " '1896.jpg': 37,\n",
       " '1897.jpg': 37,\n",
       " '1898.jpg': 37,\n",
       " '1899.jpg': 38,\n",
       " '1945.jpg': 38,\n",
       " '1946.jpg': 38,\n",
       " '1947.jpg': 34,\n",
       " '1948.jpg': 37,\n",
       " '1949.jpg': 38,\n",
       " '1995.jpg': 34,\n",
       " '1996.jpg': 35,\n",
       " '1997.jpg': 38,\n",
       " '1998.jpg': 39,\n",
       " '1999.jpg': 39,\n",
       " '2045.jpg': 40,\n",
       " '2046.jpg': 40,\n",
       " '2047.jpg': 40,\n",
       " '2048.jpg': 40,\n",
       " '2049.jpg': 40,\n",
       " '2095.jpg': 41,\n",
       " '2096.jpg': 42,\n",
       " '2097.jpg': 41,\n",
       " '2098.jpg': 1,\n",
       " '2099.jpg': 41,\n",
       " '2145.jpg': 41,\n",
       " '2146.jpg': 14,\n",
       " '2147.jpg': 40,\n",
       " '2148.jpg': 40,\n",
       " '2149.jpg': 42,\n",
       " '2195.jpg': 43,\n",
       " '2196.jpg': 43,\n",
       " '2197.jpg': 43,\n",
       " '2198.jpg': 43,\n",
       " '2199.jpg': 43,\n",
       " '2245.jpg': 45,\n",
       " '2246.jpg': 44,\n",
       " '2247.jpg': 0,\n",
       " '2248.jpg': 44,\n",
       " '2249.jpg': 44,\n",
       " '2295.jpg': 44,\n",
       " '2296.jpg': 45,\n",
       " '2297.jpg': 20,\n",
       " '2298.jpg': 44,\n",
       " '2299.jpg': 44,\n",
       " '2345.jpg': 46,\n",
       " '2346.jpg': 46,\n",
       " '2347.jpg': 46,\n",
       " '2348.jpg': 46,\n",
       " '2349.jpg': 46,\n",
       " '2395.jpg': 47,\n",
       " '2396.jpg': 47,\n",
       " '2397.jpg': 47,\n",
       " '2398.jpg': 47,\n",
       " '2399.jpg': 47,\n",
       " '2445.jpg': 7,\n",
       " '2446.jpg': 48,\n",
       " '2447.jpg': 24,\n",
       " '2448.jpg': 48,\n",
       " '2449.jpg': 48,\n",
       " '2495.jpg': 49,\n",
       " '2496.jpg': 49,\n",
       " '2497.jpg': 49,\n",
       " '2498.jpg': 49,\n",
       " '2499.jpg': 16}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = classify(model, 'test/');\n",
    "dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ResNet50(weights=\"imagenet\", include_top=False)\n",
    "model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "model = Xception(weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            filename, class_id = line.rstrip('\\n').split(',')\n",
    "            res[filename] = int(class_id)\n",
    "    return res\n",
    "\n",
    "def save_csv(img_classes, filename):\n",
    "    with open(filename, 'w') as fhandle:\n",
    "        print('filename,class_id', file=fhandle)\n",
    "        for filename in sorted(img_classes.keys()):\n",
    "            print('%s,%d' % (filename, img_classes[filename]), file=fhandle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import random\n",
    "from skimage import io\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "train_dir = '/Users/anoshin_alexey/Desktop/ComputerVision/Task5/train'\n",
    "train_gt = read_csv(join(train_dir, 'gt.csv'))\n",
    "train_img_dir = join(train_dir)\n",
    "names = list(train_gt.keys())\n",
    "random.shuffle(names)\n",
    "# os.mkdir('train')\n",
    "# os.mkdir('test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ActivityRegularization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import math\n",
    "\n",
    "import random \n",
    "from math import cos,sin\n",
    "from keras.preprocessing import image\n",
    "from scipy import misc\n",
    "from scipy.misc import imresize\n",
    "from os.path import join\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# aver = np.array([ 0.54143664, 0.43471987, 0.37830626])\n",
    "# disp = np.array([0.28285355, 0.2590644, 0.25615854])\n",
    "\n",
    "img_rows, img_cols = 256, 256\n",
    "nb_classes = 50\n",
    "def yagenerator(folder, gt, batch_size):\n",
    "    h, w = 256, 256\n",
    "    names = list(gt.keys())\n",
    "#     names = names[:5000]\n",
    "    num = len(names)\n",
    "    #x = np.empty((batch_size, img_rows, img_cols, 3))\n",
    "    #y = np.empty((batch_size, 28))\n",
    "    \n",
    "    while(True):\n",
    "        random.shuffle(names)\n",
    "        for i in range(num // batch_size - 1):\n",
    "            x = np.empty((batch_size, img_rows, img_cols, 3))\n",
    "            y = np.zeros((batch_size, nb_classes))\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                ind = i * batch_size + j\n",
    "                name = names[ind]\n",
    "                #img = imread(join(folder, names[i]))\n",
    "                img = image.load_img(join(folder, name))\n",
    "                img = image.img_to_array(img)\n",
    "                hr = 1 / img.shape[0]\n",
    "                wr = 1 / img.shape[1]\n",
    "                img = imresize(img, (h, w), interp='bilinear')\n",
    "                x[j] = (img.astype('float32'))/255.\n",
    "                y [j, gt[name]] = 1\n",
    "#                 y[j] = gt[name]\n",
    "#             flip(x, y, batch_size, 0.7)\n",
    "#             rotate(x, y, batch_size, 0.6)\n",
    "#             crop(x, y, batch_size, 0.7)\n",
    "#             for col in range(0,3):\n",
    "#                 x[:,:,:,col] -= aver[col]\n",
    "#                 x[:,:,:,col] /= disp[col]\n",
    "            yield (x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classifier(train_gt, train_img_dir, fast_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_classifier(train_gt, train_img_dir, fast_train=True):\n",
    "    nb_train_samples = 2250\n",
    "    nb_val_samples = 250\n",
    "    nb_epoch = 20\n",
    "    batch_size = 16\n",
    "    img_height, img_width =  224, 224\n",
    "    train_data_dir = '/home/ec2-user/mynotebooks/train'\n",
    "    test_data_dir = '/home/ec2-user/mynotebooks/test'\n",
    "    train_gt = read_csv(join(train_data_dir, 'gt.csv'))\n",
    "    test_gt = read_csv(join(test_data_dir, 'gt.csv'))\n",
    "\n",
    "    base_model = ResNet50(weights=\"imagenet\", input_shape=(img_height, img_width, 3), include_top=False)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # x = Dense(1024, activation='relu')(x) #new FC layer, random init\n",
    "    # predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    # model = Model(input=base_model.input, output=predictions)\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(\"compiled\")\n",
    "    hist = model.fit_generator(\n",
    "    yagenerator(train_data_dir, train_gt, batch_size=batch_size, augment=False),\n",
    "    nb_epoch=nb_epoch,\n",
    "    steps_per_epoch=nb_train_samples//batch_size,\n",
    "    validation_data=yagenerator(test_data_dir, test_gt, batch_size=1, augment=False),\n",
    "    validation_steps=nb_val_samples)\n",
    "\n",
    "    model.save(\"model_2044.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_21 to have shape (None, 50) but got array with shape (16, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-caaac6cedcd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myagenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     validation_steps=nb_validation_samples, verbose=1)\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1557\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1559\u001b[0;31m             check_batch_axis=True)\n\u001b[0m\u001b[1;32m   1560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1239\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1240\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_21 to have shape (None, 50) but got array with shape (16, 1)"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "nb_train_samples = 2250\n",
    "nb_validation_samples = 250\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "img_height, img_width =  256, 256\n",
    "nb_classes = 50\n",
    "train_data_dir = '/Users/anoshin_alexey/Desktop/ComputerVision/Task5/train'\n",
    "validation_data_dir = '/Users/anoshin_alexey/Desktop/ComputerVision/Task5/test'\n",
    "train_gt = read_csv(join(train_data_dir, 'gt.csv'))\n",
    "test_gt = read_csv(join(validation_data_dir, 'gt.csv'))\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "# build the ResNet50 network\n",
    "model = ResNet50(weights=\"imagenet\", include_top=False)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# model.layers.pop()\n",
    "# model.outputs = [model.layers[-1].output]\n",
    "# model.layers[-1].outbound_nodes = []\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x) #new FC layer, random init\n",
    "predictions = Dense(nb_classes, activation='softmax')(x) #new \n",
    "model = Model(input=model.input, output=predictions)\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_data_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     classes=50)\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#     validation_data_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     classes=test_gt )\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    yagenerator(train_data_dir, train_gt, batch_size=batch_size),\n",
    "    steps_per_epoch=nb_train_samples,\n",
    "    epochs=1,\n",
    "    validation_data=yagenerator(validation_data_dir, test_gt, batch_size=1),\n",
    "    validation_steps=nb_validation_samples, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import __version__\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "NB_EPOCHS = 3\n",
    "BAT_SIZE = 32\n",
    "FC_SIZE = 1024\n",
    "NB_IV3_LAYERS_TO_FREEZE = 172\n",
    "\n",
    "\n",
    "def get_nb_files(directory):\n",
    "  \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  cnt = 0\n",
    "  for r, dirs, files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "  return cnt\n",
    "\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model):\n",
    "  \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "  \"\"\"Add last layer to the convnet\n",
    "  Args:\n",
    "    base_model: keras model excluding top\n",
    "    nb_classes: # of classes\n",
    "  Returns:\n",
    "    new keras model with last layer\n",
    "  \"\"\"\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "  predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "  model = Model(input=base_model.input, output=predictions)\n",
    "  return model\n",
    "\n",
    "\n",
    "def setup_to_finetune(model):\n",
    "  \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "  note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "  Args:\n",
    "    model: keras model\n",
    "  \"\"\"\n",
    "  for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "     layer.trainable = False\n",
    "  for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "     layer.trainable = True\n",
    "  model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "def plot_training(history):\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  plt.plot(epochs, acc, 'r.')\n",
    "  plt.plot(epochs, val_acc, 'r')\n",
    "  plt.title('Training and validation accuracy')\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'r.')\n",
    "  plt.plot(epochs, val_loss, 'r-')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=250, class_weight=\"auto\", steps_per_epoch=2250, epochs=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2250/2250 [==============================] - 17238s - loss: 0.8937 - acc: 0.7228 - val_loss: 1.8010 - val_acc: 0.6205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=250, class_weight=\"auto\", verbose=1, steps_per_epoch=2250, epochs=1)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2250/2250 [==============================] - 24834s - loss: 0.0582 - acc: 0.9854 - val_loss: 1.3769 - val_acc: 0.6930\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naozBL0mN+X925ycPsS6NtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f6e50710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQYa7JDXIcJekBv0/UVQrISz27P8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f72a7390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_train_samples = 2250\n",
    "nb_val_samples = 250\n",
    "nb_epoch = 1\n",
    "batch_size = 16\n",
    "img_height, img_width =  256, 256\n",
    "nb_classes = 50\n",
    "train_data_dir = '/Users/anoshin_alexey/Desktop/ComputerVision/Task5/train'\n",
    "test_data_dir = '/Users/anoshin_alexey/Desktop/ComputerVision/Task5/test'\n",
    "train_gt = read_csv(join(train_data_dir, 'gt.csv'))\n",
    "test_gt = read_csv(join(validation_data_dir, 'gt.csv'))\n",
    "\n",
    "\n",
    "# setup model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False) #include_top=False excludes final FC layer\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "# transfer learning\n",
    "setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "history_tl = model.fit_generator(\n",
    "yagenerator(train_data_dir, train_gt, batch_size=batch_size),\n",
    "nb_epoch=nb_epoch,\n",
    "steps_per_epoch=nb_train_samples//batch_size,\n",
    "validation_data=yagenerator(test_data_dir, test_gt, batch_size=batch_size),\n",
    "validation_steps=nb_val_samples,\n",
    "class_weight='auto')\n",
    "\n",
    "# fine-tuning\n",
    "setup_to_finetune(model)\n",
    "\n",
    "history_ft = model.fit_generator(\n",
    "yagenerator(train_data_dir, train_gt, batch_size=batch_size),\n",
    "steps_per_epoch=nb_train_samples//batch_size,\n",
    "nb_epoch=nb_epoch,\n",
    "validation_data=yagenerator(test_data_dir, test_gt, batch_size=batch_size),\n",
    "validation_steps=nb_val_samples,\n",
    "class_weight='auto',\n",
    "verbose=1)\n",
    "\n",
    "model.save(\"model.hdf5\")\n",
    "\n",
    "plot_training(history_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
